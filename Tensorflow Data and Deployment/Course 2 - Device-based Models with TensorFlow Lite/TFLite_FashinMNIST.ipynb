{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TFLite FashinMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "coursera": {
      "course_slug": "device-based-models-tensorflow",
      "graded_item_id": "sCFzO",
      "launcher_item_id": "fJyaf"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baIUIfrChJcY",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ka96-ajYzxVU"
      },
      "source": [
        "# Train Your Own Model and Convert It to TFLite\n",
        "\n",
        "This notebook uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the articles of clothing we'll use here.\n",
        "\n",
        "This uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n",
        "\n",
        "We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rjOAfhgd__Sp"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pfyZKowNAQ4j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "b5b29e83-3d06-4a02-f9e2-f029f849e14f"
      },
      "source": [
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "\n",
        "# TensorFlow Datsets\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# Helper Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "\n",
        "from os import getcwd\n",
        "\n",
        "print('\\u2022 Using TensorFlow Version:', tf.__version__)\n",
        "print('\\u2022 GPU Device Found.' if tf.test.is_gpu_available() else '\\u2022 GPU Device Not Found. Running on CPU')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "• Using TensorFlow Version: 2.2.0\n",
            "WARNING:tensorflow:From <ipython-input-1-d6bb89964d10>:16: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "• GPU Device Found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tadPBTEiAprt"
      },
      "source": [
        "# Download Fashion MNIST Dataset\n",
        "\n",
        "We will use TensorFlow Datasets to load the Fashion MNIST dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XcNwi6nFKneZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a6a0d520-d45a-4db7-e3d4-df22be36df25"
      },
      "source": [
        "#splits = tfds.Split.ALL.subsplit(weighted=(80, 10, 10))\n",
        "\n",
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "(train_examples, validation_examples, test_examples), info = tfds.load('fashion_mnist', with_info=True, as_supervised=True, split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'], data_dir=filePath)\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset fashion_mnist/3.0.0 (download: 29.45 MiB, generated: Unknown size, total: 29.45 MiB) to /content/../tmp2/fashion_mnist/3.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /content/../tmp2/fashion_mnist/3.0.0.incompleteIAS43Y/fashion_mnist-train.tfrecord\n",
            "Shuffling and writing examples to /content/../tmp2/fashion_mnist/3.0.0.incompleteIAS43Y/fashion_mnist-test.tfrecord\n",
            "\u001b[1mDataset fashion_mnist downloaded and prepared to /content/../tmp2/fashion_mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Is76GzhJcq",
        "colab_type": "text"
      },
      "source": [
        "The class names are not included with the dataset, so we will specify them here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-eAv71FRm4JE",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt_top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hXe6jNokqX3_",
        "colab": {}
      },
      "source": [
        "# Create a labels.txt file with the class names\n",
        "with open('labels.txt', 'w') as f:\n",
        "    f.write('\\n'.join(class_names))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iubWCThbdN8K",
        "colab": {}
      },
      "source": [
        "# The images in the dataset are 28 by 28 pixels.\n",
        "IMG_SIZE = 28"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZAkuq0V0Aw2X"
      },
      "source": [
        "# Preprocessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_5SIivkunKCC"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI9cm5fKhJc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Write a function to normalize the images.\n",
        "\n",
        "def format_example(image, label):\n",
        "    # Cast image to float32\n",
        "    image = tf.cast(image, tf.float32)\n",
        "        \n",
        "    # Normalize the image in the range [0, 1]\n",
        "    image = image/255\n",
        "    \n",
        "    return image, label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HAlBlXOUMwqe",
        "colab": {}
      },
      "source": [
        "# Specify the batch size\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JM4HfIJtnNEk"
      },
      "source": [
        "## Create Datasets From Images and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UiE1cGghJdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Datasets\n",
        "train_batches = train_examples.cache().shuffle(num_examples//4).batch(BATCH_SIZE).map(format_example).prefetch(1)\n",
        "validation_batches = validation_examples.cache().batch(BATCH_SIZE).map(format_example)\n",
        "test_batches = test_examples.map(format_example).batch(1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M-topQaOm_LM"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeaNwp16hJdF",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
        "_________________________________________________________________\n",
        "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
        "_________________________________________________________________\n",
        "flatten (Flatten)            (None, 3872)              0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 64)                247872    \n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)              (None, 10)                650       \n",
        "=================================================================\n",
        "Total params: 253,322\n",
        "Trainable params: 253,322\n",
        "Non-trainable params: 0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEUk7QdehJdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Build and compile the model shown in the previous cell.\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # Set the input shape to (28, 28, 1), kernel size=3, filters=16 and use ReLU activation,\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), input_shape=(28, 28, 1), activation='relu'),\n",
        "      \n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "      \n",
        "    # Set the number of filters to 32, kernel size to 3 and use ReLU activation \n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "      \n",
        "    # Flatten the output layer to 1 dimension\n",
        "    tf.keras.layers.Flatten(),\n",
        "      \n",
        "    # Add a fully connected layer with 64 hidden units and ReLU activation\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "      \n",
        "    # Attach a final softmax classification head\n",
        "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Set the appropriate loss function and use accuracy as your metric\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zEMOz-LDnxgD"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JGlNoRtzCP4_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "1e89851e-b841-47de-a04d-12a0e0f293f8"
      },
      "source": [
        "history = model.fit(train_batches, epochs=10, validation_data=validation_batches)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "188/188 [==============================] - 8s 41ms/step - loss: 0.5856 - accuracy: 0.7981 - val_loss: 0.3889 - val_accuracy: 0.8622\n",
            "Epoch 2/10\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3754 - accuracy: 0.8678 - val_loss: 0.3350 - val_accuracy: 0.8775\n",
            "Epoch 3/10\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3254 - accuracy: 0.8856 - val_loss: 0.3271 - val_accuracy: 0.8818\n",
            "Epoch 4/10\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2985 - accuracy: 0.8915 - val_loss: 0.3072 - val_accuracy: 0.8872\n",
            "Epoch 5/10\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2718 - accuracy: 0.9013 - val_loss: 0.2791 - val_accuracy: 0.8988\n",
            "Epoch 6/10\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2575 - accuracy: 0.9068 - val_loss: 0.2601 - val_accuracy: 0.9052\n",
            "Epoch 7/10\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2415 - accuracy: 0.9124 - val_loss: 0.2576 - val_accuracy: 0.9077\n",
            "Epoch 8/10\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2282 - accuracy: 0.9177 - val_loss: 0.2454 - val_accuracy: 0.9110\n",
            "Epoch 9/10\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2159 - accuracy: 0.9216 - val_loss: 0.2393 - val_accuracy: 0.9135\n",
            "Epoch 10/10\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2063 - accuracy: 0.9249 - val_loss: 0.2541 - val_accuracy: 0.9063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TZT9-7w9n4YO"
      },
      "source": [
        "# Exporting to TFLite\n",
        "\n",
        "You will now save the model to TFLite. We should note, that you will probably see some warning messages when running the code below. These warnings have to do with software updates and should not cause any errors or prevent your code from running. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfvdoWlhJdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "6596a8c7-884f-4570-8723-027945b0e828"
      },
      "source": [
        "# EXERCISE: Use the tf.saved_model API to save your model in the SavedModel format. \n",
        "export_dir = 'saved_model/1'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "tf.saved_model.save(model, export_dir)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "EDGiYrBdE6fl",
        "colab": {}
      },
      "source": [
        "# Select mode of optimization\n",
        "mode = \"Speed\" \n",
        "\n",
        "if mode == 'Storage':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
        "elif mode == 'Speed':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
        "else:\n",
        "    optimization = tf.lite.Optimize.DEFAULT"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzP6oFMBhJdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Use the TFLiteConverter SavedModel API to initialize the converter\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "\n",
        "# Set the optimzations\n",
        "converter.optimizations = [optimization]\n",
        "\n",
        "# Invoke the converter to finally generate the TFLite model\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q5PWCDsTC3El",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "570ae85b-c02e-4942-c7b6-b2c5354c6025"
      },
      "source": [
        "tflite_model_file = pathlib.Path('./model.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "258672"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SR6wFcQ1Fglm"
      },
      "source": [
        "# Test the Model with TFLite Interpreter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rKcToCBEC-Bu",
        "colab": {}
      },
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E8EpFpIBFkq8",
        "colab": {}
      },
      "source": [
        "# Gather results for the randomly sampled test images\n",
        "predictions = []\n",
        "test_labels = []\n",
        "test_images = []\n",
        "\n",
        "for img, label in test_batches.take(50):\n",
        "    interpreter.set_tensor(input_index, img)\n",
        "    interpreter.invoke()\n",
        "    predictions.append(interpreter.get_tensor(output_index))\n",
        "    test_labels.append(label[0])\n",
        "    test_images.append(np.array(img))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "kSjTmi05Tyod",
        "colab": {}
      },
      "source": [
        "# Utilities functions for plotting\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    img = np.squeeze(img)\n",
        "    \n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "    \n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    \n",
        "    if predicted_label == true_label.numpy():\n",
        "        color = 'green'\n",
        "    else:\n",
        "        color = 'red'\n",
        "        \n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                         100*np.max(predictions_array),\n",
        "                                         class_names[true_label]),\n",
        "                                         color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "    predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks(list(range(10)))\n",
        "    plt.yticks([])\n",
        "    thisplot = plt.bar(range(10), predictions_array[0], color=\"#777777\")\n",
        "    plt.ylim([0, 1])\n",
        "    predicted_label = np.argmax(predictions_array[0])\n",
        "    \n",
        "    thisplot[predicted_label].set_color('red')\n",
        "    thisplot[true_label].set_color('blue')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "ZZwg0wFaVXhZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "d390810d-fa6b-4c8c-ec20-968e57a0db94"
      },
      "source": [
        "# Visualize the outputs\n",
        "\n",
        "# Select index of image to display. Minimum index value is 1 and max index value is 50. \n",
        "index = 49 \n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(index, predictions, test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(index, predictions, test_labels)\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD0IAnQShJdy",
        "colab_type": "text"
      },
      "source": [
        "# Click the Submit Assignment Button Above\n",
        "\n",
        "You should now click the Submit Assignment button above to submit your notebook for grading. Once you have submitted your assignment, you can continue with the optinal section below. \n",
        "\n",
        "## If you are done, please **don't forget to run the last two cells of this notebook** to save your work and close the Notebook to free up resources for your fellow learners. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H8t7_jRiz9Vw"
      },
      "source": [
        "# Prepare the Test Images for Download (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fi09nIps0gBu",
        "colab": {}
      },
      "source": [
        "!mkdir -p test_images"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sF7EZ63J0hZs",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "for index, (image, label) in enumerate(test_batches.take(50)):\n",
        "    image = tf.cast(image * 255.0, tf.uint8)\n",
        "    image = tf.squeeze(image).numpy()\n",
        "    pil_image = Image.fromarray(image)\n",
        "    pil_image.save('test_images/{}_{}.jpg'.format(class_names[label[0]].lower(), index))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uM35O-uv0iWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "50076c29-8335-4e23-b65e-979004b2ddfa"
      },
      "source": [
        "!ls test_images"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'ankle boot_10.jpg'   coat_40.jpg       sandal_19.jpg\t sneaker_43.jpg\n",
            "'ankle boot_32.jpg'   coat_46.jpg       sandal_2.jpg\t trouser_20.jpg\n",
            "'ankle boot_4.jpg'    coat_48.jpg       sandal_39.jpg\t trouser_22.jpg\n",
            " bag_16.jpg\t      dress_12.jpg      shirt_27.jpg\t trouser_35.jpg\n",
            " bag_17.jpg\t      dress_29.jpg      shirt_33.jpg\t trouser_49.jpg\n",
            " bag_23.jpg\t      dress_37.jpg      shirt_5.jpg\t t-shirt_top_15.jpg\n",
            " bag_34.jpg\t      dress_45.jpg      sneaker_13.jpg\t t-shirt_top_18.jpg\n",
            " bag_36.jpg\t      dress_6.jpg       sneaker_24.jpg\t t-shirt_top_1.jpg\n",
            " bag_3.jpg\t      pullover_28.jpg   sneaker_25.jpg\t t-shirt_top_21.jpg\n",
            " bag_7.jpg\t      pullover_44.jpg   sneaker_26.jpg\t t-shirt_top_47.jpg\n",
            " coat_11.jpg\t      pullover_9.jpg    sneaker_38.jpg\t t-shirt_top_8.jpg\n",
            " coat_30.jpg\t      sandal_0.jpg      sneaker_41.jpg\n",
            " coat_31.jpg\t      sandal_14.jpg     sneaker_42.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aR20r4qW0jVm",
        "colab": {}
      },
      "source": [
        "!tar --create --file=fmnist_test_images.tar test_images"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er_K3nYehJeG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0deca614-6e04-490c-898d-580b1adc17c5"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fmnist_test_images.tar\tmodel.tflite  saved_model\n",
            "labels.txt\t\tsample_data   test_images\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}